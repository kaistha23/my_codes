# -*- coding: utf-8 -*-
"""kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Weni-SJxTT6YoSxNPXd9_f6ISuSeUvNr

**Interview questions to test data science skills**

---

# Let's load the data from public GitHub account
"""

import pandas as pd
import numpy as np

df = pd.read_csv("https://github.com/manjiler/interview_for_datascience/raw/master/interview_df.csv",parse_dates=["datetimeindex"])

df.head(10)

"""# Now that the data is loaded. let's begin!!!

# **About the data**

---
This is time series data for one month (May month) collected for 7 different sensors. The column "output" is the response of whole system. 

"output" should be predicted a function of the sensors

## Q1. Do an EDA on the data, correlation plots, features that might be important for the modeling. Share your observations. Comment on how the data looks from modeling perspective
"""

df.info()

df.isnull().sum()

df.describe()

df['datetimeindex'].min()

df['datetimeindex'].max()

interval = 12*24*30
interval

df['datetimeindex'] = pd.to_datetime(df['datetimeindex'], format = '%Y/%m/%d %H:%M:%S')

df['weekday'] = df['datetimeindex'].dt.day_name()
df.head()

hours = pd.to_datetime(df['datetimeindex'], format='%H:%M:%S').dt.hour

df['hour_bin'] = pd.cut(hours, 
                    bins=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24], 
                    include_lowest=True, 
                    labels=['hr_1','hr_2','hr_3','hr_4','hr_5','hr_6','hr_7','hr_8','hr_9','hr_10','hr_11','hr_12','hr_13','hr_14','hr_15','hr_16','hr_17','hr_18','hr_19','hr_20','hr_21','hr_22','hr_23','hr_24'])

df.head()

df['time'] = pd.to_datetime(df['datetimeindex'])
df['dates'] = df['time'].dt.date

df.head()

# Convert string to datetime64
df['datetimeindex'] = df['datetimeindex'].apply(pd.to_datetime)
df.set_index('datetimeindex',inplace=True)
df.head()

df['WEEKEND'] = np.where(df.time.dt.dayofweek.isin([5,6]), 1, 0)

df.head()



print(df.index.max())
print(df.index.min())

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %matplotlib inline
from matplotlib import pyplot as plt


df['output'].plot(figsize=(10,6),title='output')
plt.ylabel('output')

var = ["s1","s2","s3","s4","s5","s6","s7"]
df[var].plot(subplots=True, figsize=(20,30))
plt.title('S1-S7 attributes in May')
plt.show()

## No Trend and Seasonality

df.skew()

"""If the skewness is between -0.5 and 0.5, the data are fairly symmetrical
If the skewness is between -1 and – 0.5 or between 0.5 and 1, the data are moderately skewed
If the skewness is less than -1 or greater than 1, the data are highly skewed

"""

df.kurt()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %matplotlib inline
from matplotlib import pyplot as plt
import seaborn as sns

var = ["s1","s2","s3","s4","s5","s6","s7"]
# Box Plot
plt.figure(figsize=(10,10))
plt.title('Box Plot')
ax = sns.boxenplot(data=df[var], palette="Set2")

## S3 & S6 outliers

g = sns.PairGrid(df)
g.map_upper(sns.histplot)
g.map_lower(sns.kdeplot, fill=True)
g.map_diag(sns.histplot, kde=True)

g = sns.PairGrid(df)
#g.map_upper(sns.histplot)
g.map_lower(sns.kdeplot, fill=True)
g.map_diag(sns.histplot, kde=True)

sns.pairplot(df, x_vars=["s1","s2","s3","s4","s5","s6","s7"], y_vars=["output"],
             height=5, aspect=.8, kind="reg");

mask = np.tril(df.corr())
sns.heatmap(df.corr(), annot = True, fmt='.1g', mask=mask)

## Log Transformed

#df['s3'] = np.log(df[‘price’])
import scipy.stats as stats
sns.distplot(np.log(df['s3']))
fig = plt.figure()

#df['s3'] = np.log(df[‘price’])
import scipy.stats as stats
sns.distplot(np.log(df['s6']))
fig = plt.figure()

## Log transform
df['s3'] = np.log(df['s3'])
df['s6'] = np.log(df['s6'])

f= plt.figure(figsize=(12,4))

ax=f.add_subplot(121)
sns.distplot(df['s3'],color='r',ax=ax)
ax.set_title('Distribution of raw S3')

ax=f.add_subplot(122)
sns.distplot(np.log(df['s3']),color='b',ax=ax)
ax.set_title('Distribution of S3 in $log$ sacle')

ax = sns.regplot(x="s6", y="output", data=df)
ax.set_title('Distribution of S6 in $log$ sacle')

pt = df.groupby(['hour_bin']).mean()['output']
# plot the result
pt.plot()
plt.xticks(rotation=45)
plt.show()

"""No Corr, n No Multi Colebraity """

pt = df.groupby(['weekday']).mean()['output']
 
# plot the result
pt.plot()
plt.xticks(rotation=45)
plt.show()



# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %matplotlib inline
from matplotlib import pyplot as plt

df.columns

dum = ['WEEKEND','weekday','hour_bin']
#df_tr1.head()
df_dum = pd.get_dummies(data=df, prefix_sep='_', columns= dum)
mask = np.tril(df_dum.corr())
plt.figure(figsize=(30,30))
sns.heatmap(df_dum.corr(), annot = True, fmt='.1g', mask=mask)



"""## Q2. Check for any seasonality in data. Plot a rolling mean and resample mean for each of the sensors."""

# Stacked line plot
plt.figure(figsize=(10,10))
plt.title('Weekday Seasonality of the Time Series')
sns.pointplot(x='hour_bin',y='output',data=df)

# Stacked line plot
plt.figure(figsize=(10,10))
plt.title('Weekday Seasonality of the Time Series')
sns.pointplot(x='weekday',y='output',data=df)

# Stacked line plot
plt.figure(figsize=(10,10))
plt.title('Weekday Seasonality of the Time Series')
sns.pointplot(x='WEEKEND',y='output',data=df)

df.index

fig, ax = plt.subplots(1,3, figsize=(20,10), sharey=True)
df["s3"].asfreq('H').plot(ax=ax[0]) # asfreq method is used to convert a time series to a specified frequency. Here it is monthly frequency.
df["s3"].asfreq('D').plot(ax=ax[1]) # asfreq method is used to convert a time series to a specified frequency. Here it is monthly frequency.
df["s3"].asfreq('W').plot(ax=ax[2]) # asfreq method is used to convert a time series to a specified frequency. Here it is monthly frequency.
plt.title('output over time Hourly, daily & Weekly frequency')
plt.show()

"""### Rolling mean"""

fig, ax = plt.subplots(figsize=(10,12), sharey=True)
roll_s1 = df.s1.rolling('1D').mean()
df.s1.plot()
roll_s1.plot()
plt.legend(['s1','Rolling Mean'])
# Plotting a rolling mean of 90 day window with original High attribute of google stocks
plt.show()

"""Mean is almost zero

# Resample

### Hour wise Rolling & Resample Mean
"""

fig, ax = plt.subplots(figsize=(10,6), sharey=True)
roll_s1 = df.s1.rolling('1H').mean()



roll_s1.plot()
plt.legend(['Hourly Rolling Mean','Daily Resample Mean'])
#plt.legend(['s1','Rolling Mean'])
# Plotting a rolling mean of 90 day window with original High attribute of google stocks
plt.show()

fig, ax = plt.subplots(figsize=(10,6), sharey=True)

re_s1 = df.s1.resample('5T').mean()
#df.s1.plot()

re_s1.plot()

plt.legend(['Hourly Rolling Mean','Daily Resample Mean'])
#plt.legend(['s1','Rolling Mean'])
# Plotting a rolling mean of 90 day window with original High attribute of google stocks
plt.show()

fig, ax = plt.subplots(figsize=(10,6), sharey=True)

re_s1 = df.s1.resample('1H').mean()
#df.s1.plot()

re_s1.plot()

plt.legend(['Hourly Rolling Mean','Daily Resample Mean'])
#plt.legend(['s1','Rolling Mean'])
# Plotting a rolling mean of 90 day window with original High attribute of google stocks
plt.show()

Rolling Mean no seasinalty , no spikes , no adavngate

"""### Day wise Rolling & Resample Mean"""

fig, ax = plt.subplots(figsize=(10,6), sharey=True)
roll_s1 = df.s1.rolling('1D').mean()
re_s1 = df.s1.resample('1D').mean()
print(df['s1'].head())
print(roll_s1.head())
print(re_s1.head())
re_s1.plot()
roll_s1.plot()
plt.legend(['Daily Rolling Mean','Daily Resample Mean'])
plt.show()

#Is the mean constant?
df["output"].plot(figsize=(12,6), legend=True, label="output")
df["output"].rolling('1H', center=False).mean().plot(legend=True, label="Rolling Mean 4Q");
print("Mean is:", df["output"].mean())
print("Mean is:", df["output"].rolling('1H').mean())

#Is the variance constant?
df["output"].plot(figsize=(12,6), legend=True, label="output")
df["output"].rolling('1H', center=False).std().plot(legend=True, label="Rolling Mean 4Q");
print("Std is:", df["output"].std())
print("Mean is:", df["output"].rolling('1H').std())

df["output"].plot(figsize=(12,6), legend=True, label="output")
df["output"].rolling('1H', center=False).mean().plot(legend=True, label="Rolling Mean 4Q");
print("Mean is:", df["output"].mean())

If Meaningfull
#c.v = s.d/mean

#If C.V<0.75 => Low Variability

#If 0.75<C.V<1.3 => Medium Variability

#If C.V>1.3 => High Variability

cv = df["output"].std()/df["output"].mean()
cv

# No 
sm.graphics.tsa.plot_acf(df['output']);
sm.graphics.tsa.plot_pacf(df['output']);

pd.plotting.lag_plot(df['output'],1);

plt.rcParams['figure.figsize'] = (8, 6)
df['s1'].resample('D').mean().plot(kind='bar')
plt.title('s1 - Daily Mean Shift')

fig, ax = plt.subplots(1,2, figsize=(10,5), sharey=True)
sns.color_palette()

plt.title('Seasonality of the Time Series')
sns.pointplot(ax=ax[0], x='weekday',y='s1',data=df, label ='S1')
sns.pointplot(ax=ax[1], x='weekday',y='s2',data=df,  label ='S2',  palette="Greens")
sns.pointplot(ax=ax[1], x='weekday',y='s3',data=df ,  label ='S3', palette="Reds")

# Stacked line plot
plt.figure(figsize=(10,10))
plt.title('Seasonality of the Time Series')
sns.pointplot(x='weeknum',y='s1',data=df)

f= plt.figure(figsize=(12,4))

ax=f.add_subplot(121)
sns.distplot(df['s3'],bins=50,color='r',ax=ax)
ax.set_title('Distribution of raw S3')

ax=f.add_subplot(122)
sns.distplot(np.log(df['s3']),bins=10,color='b',ax=ax)
ax.set_title('Distribution of S3 in $log$ sacle')
ax.set_xscale('log');



"""## Q3. Fit a linear regression to this data with y as "output" column. Comment on the fit of the model"""

import statsmodels.api as sm

#df = pd.read_csv("https://github.com/manjiler/interview_for_datascience/raw/master/interview_df.csv",parse_dates=["datetimeindex"])

df.columns

from sklearn.linear_model import LinearRegression
# linear model
v = ['s1', 's2', 's3' , 's4' , 's5', 's6', 's7']
x= df.drop(['timestamp','weekday', 'hour_bin', 'time', 'dates', 'WEEKEND'], axis = 1)
y= df.output

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y,random_state = 5 ,test_size=0.3)

X_endog = sm.add_constant(X_train)

res = sm.OLS(y_train, X_endog)

model = res.fit()
model.summary()

model_fitted_y = model_fit.fittedvalues
# model residuals
model_residuals = model_fit.resid
# normalized residuals
model_norm_residuals = model_fit.get_influence().resid_studentized_internal
# absolute squared normalized residuals
model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))
# absolute residuals
model_abs_resid = np.abs(model_residuals)
# leverage, from statsmodels internals
model_leverage = model_fit.get_influence().hat_matrix_diag
# cook's distance, from statsmodels internals
model_cooks = model_fit.get_influence().cooks_distance[0]

plot_lm_1 = plt.figure()
plot_lm_1.axes[0] = sns.residplot(model_fitted_y, dataframe.columns[-1], data=dataframe,
                          lowess=True,
                          scatter_kws={'alpha': 0.5},
                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})

plot_lm_1.axes[0].set_title('Residuals vs Fitted')
plot_lm_1.axes[0].set_xlabel('Fitted values')
plot_lm_1.axes[0].set_ylabel('Residuals');

from statsmodels.compat import lzip
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import ols
fig = sm.graphics.plot_partregress_grid(model)
#fig.tight_layout(pad=1.0)

ex, linear_plot = Plot.LinearRegressionResidualPlot(x_train.values, y_train.values), lm = linear_plot.fit() , summary, diag_res = linear_plot.diagnostic_plots(lm)

from statsmodels.stats.outliers_influence import variance_inflation_factor
# VIF dataframe
vif_data = pd.DataFrame()
vif_data["feature"] = df[var].columns
  
# calculating VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(df[var].values, i)
                          for i in range(len(df[var].columns))]
  
print(vif_data)

"""Interpreting the Regression Results


Adjusted. R-squared reflects the fit of the model. R-squared values range from 0 to 1, where a higher value generally indicates a better fit, assuming certain conditions are met.

const coefficient is your Y-intercept. It means that if both the Interest_Rate and Unemployment_Rate coefficients are zero, then the expected output (i.e., the Y) would be equal to the const coefficient.


std err reflects the level of accuracy of the coefficients. The lower it is, the higher is the level of accuracy

P >|t| is your p-value. A p-value of less than 0.05 is considered to be statistically significant

Confidence Interval represents the range in which our coefficients are likely to fall (with a likelihood of 95%)
"""

from scipy import stats
import seaborn as sns
stats.probplot(df['s3'], plot=sns.mpl.pyplot)

"""The image above shows quantiles from a theoretical normal distribution on the horizontal axis. It’s being compared to a set of data on the y-axis. This particular type of Q Q plot is called a normal quantile-quantile (QQ) plot. The points are not clustered on the 45 degree line, and in fact follow a curve, suggesting that the sample data is not normally distributed."""

v = ['s1', 's2', 's3' , 's4' , 's5', 's6', 's7']
x= df.drop([ 'output', 'timestamp', 'weekday', 'hour_bin', 'time', 'dates', 'WEEKEND'], axis = 1)
y= df.output

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
scaled = sc.fit_transform(x)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(scaled, y ,random_state = 5,test_size=0.3)

X_endog = sm.add_constant(X_train)

res = sm.OLS(y_train, X_endog)

model1   = res.fit()
model1.summary()

# model values
model_fitted_y = model1.fittedvalues
# model residuals
model_residuals = model1.resid
# normalized residuals
model_norm_residuals = model1.get_influence().resid_studentized_internal
# absolute squared normalized residuals
model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))
# absolute residuals
model_abs_resid = np.abs(model_residuals)
# leverage, from statsmodels internals
model_leverage = model1.get_influence().hat_matrix_diag
# cook's distance, from statsmodels internals
model_cooks = model1.get_influence().cooks_distance[0]

y_pred = model1.predict(X_endog)
residuals = y_train.values-y_pred

p = sns.scatterplot(y_pred,model_residuals)
plt.xlabel('y_pred/predicted values')
plt.ylabel('Residuals')
plt.ylim(-10,10)
plt.xlim(0,26)
p = sns.lineplot([0,26],[0,0],color='blue')
p = plt.title('Residuals vs fitted values plot for homoscedasticity check')

#Check for Normality of error terms/residuals
p = sns.distplot(residuals,kde=True)
p = plt.title('Normality of error terms/residuals')

from statsmodels.graphics.gofplots import ProbPlot
QQ = ProbPlot(model_norm_residuals)
plot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)
plot_lm_2.axes[0].set_title('Normal Q-Q')
plot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')
plot_lm_2.axes[0].set_ylabel('Standardized Residuals');



"""# Log transformed"""

## Log transform
df['s3'] = np.log(df['s3'])
df['s6'] = np.log(df['s6'])

v = ['s1', 's2', 's3' , 's4' , 's5', 's6', 's7']
x= df.drop([ 'output', 'timestamp', 'weekday', 'hour_bin', 'time', 'dates', 'WEEKEND'], axis = 1)
y= df.output

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y ,random_state = 5,test_size=0.3)

X_endog = sm.add_constant(X_train)

res = sm.OLS(y_train, X_endog)

res.fit().summary()



from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn import linear_model

regr = linear_model.LinearRegression()
regr.fit(X_train,y_train)
y_pred = regr.predict(X_train)
predictions = regr.predict(X_test)

residuals = y_train.values-y_pred
y_pred = regr.predict(X_train)
mean_residuals = np.mean(residuals)
print("Mean of Residuals {}".format(mean_residuals))

r2 = r2_score(y_true=y_train,y_pred=y_pred)
print("R Squared = ",r2)



p = sns.scatterplot(y_pred,residuals)
plt.xlabel('y_pred/predicted values')
plt.ylabel('Residuals')
plt.ylim(-10,10)
plt.xlim(0,26)
p = sns.lineplot([0,26],[0,0],color='blue')
p = plt.title('Residuals vs fitted values plot for homoscedasticity check')

#Check for Normality of error terms/residuals
p = sns.distplot(residuals,kde=True)
p = plt.title('Normality of error terms/residuals')

# We want to know what is the distribution of the residuals. 
sns.distplot(y_test-predictions)



from sklearn import metrics
y_test_pred = regr.predict(X_test)
print("MAE:", metrics.mean_absolute_error(y_test, y_test_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_test_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))

"""# Scaled Model"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
scaled = sc.fit_transform(x)

X

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(scaled, y ,random_state = 5,test_size=0.3)

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn import linear_model

regr_sc = linear_model.LinearRegression()
regr_sc.fit(X_train,y_train)
y_pred = regr_sc.predict(X_train)

print("R squared: {}".format(r2_score(y_true=y_train,y_pred=y_pred)))

from sklearn import metrics
y_test_pred = regr_sc.predict(X_test)
print("MAE:", metrics.mean_absolute_error(y_test, y_test_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_test_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))

print(regr_sc.intercept_)

residuals = y_train.values-y_pred
mean_residuals = np.mean(residuals)
print("Mean of Residuals {}".format(mean_residuals))
p = sns.scatterplot(y_pred,residuals)
plt.xlabel('y_pred/predicted values')
plt.ylabel('Residuals')
plt.ylim(-10,10)
plt.xlim(0,26)
p = sns.lineplot([0,26],[0,0],color='blue')
p = plt.title('Residuals vs fitted values plot for homoscedasticity check')

# get importance
importance = regr_sc.coef_
importance

# Commented out IPython magic to ensure Python compatibility.
coeff_df = pd.DataFrame(regr_sc.coef_, x.columns, columns=['Coefficient'])
print(coeff_df)
# %matplotlib inline
from matplotlib import pyplot as plt
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

y_pred = rf_new.predict(X_test)
x_ax = range(len(y_test))
plt.plot(x_ax, y_test, label="original")
plt.plot(x_ax, y_pred, label="predicted")
plt.title("Output  test and predicted data")
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.legend(loc='best',fancybox=True, shadow=True)
plt.grid(True)
plt.show()



"""## Q4. Fit a simple decision tree regressor to this data. Comment about the fit of the model"""

dec_tree = DecisionTreeRegressor()
dec_tree

from sklearn.tree import DecisionTreeRegressor

dec_tree = DecisionTreeRegressor(random_state=5)
dec_tree.fit(X_train,y_train)
dec_tree_y_pred = dec_tree.predict(X_train)
y_pred = dec_tree.predict(X_test)
print("Accuracy: {}".format(dec_tree.score(X_train,y_train)))
print("R squared: {}".format(r2_score(y_true=y_train,y_pred=dec_tree_y_pred)))
print("R squared: Test {}".format(r2_score(y_true=y_test,y_pred=y_pred)))

Overfitting

x_ax = range(len(y_test))
plt.plot(x_ax, y_test, label="original")
plt.plot(x_ax, y_pred, label="predicted")
plt.title("Output  test and predicted data")
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.legend(loc='best',fancybox=True, shadow=True)
plt.grid(True)
plt.show()

coeff_df = pd.DataFrame(dec_tree.feature_importances_, x.columns, columns=['Coefficient'])
print(coeff_df)
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

"""## Q5. Fit a Random forest regressor. Compare this with simple dicision tree. If Random forest is better then why"""

from sklearn.ensemble import RandomForestRegressor

rf_tree = RandomForestRegressor(random_state=5)
rf_tree.fit(X_train,y_train)
rf_tree_y_pred = rf_tree.predict(X_train)
y_pred = rf_tree.predict(X_test)
print("Accuracy: {}".format(rf_tree.score(X_train,y_train)))
print("R squared: {}".format(r2_score(y_true=y_train,y_pred=rf_tree_y_pred)))
print("R squared: Test {}".format(r2_score(y_true=y_test,y_pred=y_pred)))

"""## Q6. How do improve the accuracy of Random forest regressor"""

rf_new = RandomForestRegressor()
rf_new

rf_new = RandomForestRegressor(n_estimators = 100, criterion = 'mae', 
                               min_samples_split = 2, min_samples_leaf = 1)

rf_new.fit(X_train,y_train)
rf_tree_y_pred = rf_new.predict(X_train)
print("Accuracy: {}".format(rf_new.score(X_train,y_train)))
print("R squared: {}".format(r2_score(y_true=y_train,y_pred=rf_tree_y_pred)))

y_pred = rf_new.predict(X_test)
x_ax = range(len(y_test))
plt.plot(x_ax, y_test, label="original")
plt.plot(x_ax, y_pred, label="predicted")
plt.title("Output  test and predicted data")
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.legend(loc='best',fancybox=True, shadow=True)
plt.grid(True)
plt.show()

"""## Q7. Generate a random distribution of samples from data such that each day should contain 12 continous samples and start of the sample should be random with that day"""

lst = np.unique(df['dates'])
lst

for i in lst:
  sam = random.sample(df[var],1)

"""## Q8. Cluster the input variables using KMeans and GMM.
       
1.   Draw the contour plots
2.   Explain the hyper-parameters you choose and why?


"""

df.columns

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline 
from sklearn.cluster import KMeans

var = ['s1', 's2', 's3', 's4', 's5', 's6', 's7']

distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(df[var])
    distortions.append(kmeanModel.inertia_)

plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

kmeans

#Import required module
from sklearn.cluster import KMeans
X= df[var].values

kmeans = KMeans(n_clusters=4)
pred_y = kmeans.fit_predict(X)

sns.set_style("white")
plt.figure(figsize=(12, 6))
sns.scatterplot(x=X[:,0], y=X[:,1], s=80, alpha=.7)
sns.scatterplot(x=kmeans.cluster_centers_[:, 0], y=kmeans.cluster_centers_[:, 1], s=600)

ax1 = df.plot(kind='scatter', x='s1', y='b', color='r')    


print(ax1 == ax2 == ax3)

#plot data with seaborn
clustering_kmeans = KMeans(n_clusters=4)
df['clusters'] = clustering_kmeans.fit_predict(df[var])

facet = sns.lmplot(data=df, x='s1', y='s3', hue='clusters', 
                   fit_reg=False, legend=True, legend_out=True)

sns.catplot(x="clusters", y="output", data=df)

#plot data with seaborn


facet = sns.scatterplot(data=df, x='s1', y='s3', hue='clusters')

## Log transform
np.log(df['s3'])
np.log(df['s6'])
from sklearn.decomposition import PCA
X= df[var].values
pca = PCA(2)
X2 = pca.fit_transform(X)

kmeans = KMeans(n_clusters=3)
pred_y = kmeans.fit_predict(X2)

sns.set_style("white")
plt.figure(figsize=(12, 6))
sns.scatterplot(x=X2[:,0], y=X2[:,1], s=200, alpha=.7)
sns.scatterplot(x=kmeans.cluster_centers_[:, 0], y=kmeans.cluster_centers_[:, 1], s=600)

df.head()

panda_df = pd.DataFrame(data = pred_y, 
                        columns = ["cluster"])

dft = pd.DataFrame(data = X2, 
                        columns = ["PCA1", "PCA2"])

dft = pd.DataFrame(X2)
dft1 = pd.DataFrame(pred_y)
dft.info()
dft1.info()

dataframe = pd.concat([dft, panda_df], axis=1).reset_index()

dataframe.info()

dataframe.head()

#plot data with seaborn


facet = sns.lmplot(data=dataframe, x='PCA1', y='PCA2', hue='cluster', 
                   fit_reg=False, legend=True, legend_out=True)

facet = sns.scatterplot(data=dataframe, x='PCA1', y='PCA2', hue='cluster')

#plot data with seaborn
df['clusters'] = clustering_kmeans.fit_predict(df[var])
facet = sns.lmplot(data=data, x='X', y='y', hue='label', 
                   fit_reg=False, legend=True, legend_out=True)

## Interetation 

Scatter plot between featrurs

from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist

def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):
    labels = kmeans.fit_predict(X)

    # plot the input data
    ax = ax or plt.gca()
    ax.axis('equal')
    ax.scatter(X[:, 0], X[:, 1], c=labels, s=100, cmap='viridis', zorder=2)

    # plot the representation of the KMeans model
    centers = kmeans.cluster_centers_
    radii = [cdist(X[labels == i], [center]).max()
             for i, center in enumerate(centers)]
    for c, r in zip(centers, radii):
        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.7, zorder=1))
plot_kmeans(kmeans, X)

frame = pd.DataFrame(X2)
frame

gmm

from sklearn import mixture
gmm = mixture.GaussianMixture(n_components=2).fit(X)
labels = gmm.predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');

from sklearn.decomposition import PCA
X= df[var].values
pca = PCA(2)
X2 = pca.fit_transform(X)

gmm = mixture.GaussianMixture(n_components=2).fit(X2)
pred_y = gmm.predict(X2)

sns.set_style("white")
plt.figure(figsize=(12, 6))
sns.scatterplot(x=X2[:,0], y=X2[:,1], s=200, alpha=.7)
sns.scatterplot(x=kmeans.cluster_centers_[:, 0], y=kmeans.cluster_centers_[:, 1], s=600)

panda_df = pd.DataFrame(data = pred_y, 
                        columns = ["cluster"])

dft = pd.DataFrame(data = X2, 
                        columns = ["PCA1", "PCA2"])

panda_df = pd.DataFrame(data = pred_y, 
                        columns = ["cluster"])

dft = pd.DataFrame(data = X2, 
                        columns = ["PCA1", "PCA2"])
dataframe = pd.concat([dft, panda_df], axis=1).reset_index()

facet = sns.lmplot(data=dataframe, x='PCA1', y='PCA2', hue='cluster', 
                   fit_reg=False, legend=True, legend_out=True)